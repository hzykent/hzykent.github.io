<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zeyu Hu</title>
  
  <meta name="author" content="Zeyu Hu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zeyu Hu 胡泽宇</name>
              </p>
              <p> A senior researcher at <a href="https://www.lightspeed-studios.com/">LightSpeed Studios Singapore</a>, feel free to reach out. I obtained my Ph.D. degree (2018-2023) at the <a href="http://visgraph.cse.ust.hk/">Vision and Graphics Group</a>, part of the Department of Computer Science and Engineering at <a href="https://www.ust.hk/">the Hong Kong University of Science and Technology</a>. My Ph.D. advisor was <a href="http://www.cse.ust.hk/~taicl/">Prof. Chiew-Lan Tai</a>, and I also worked closely with <a href="https://www.cse.ust.hk/~quan/">Prof. Long Quan</a> and <a href="http://sweb.cityu.edu.hk/hongbofu/">Prof. Hongbo Fu</a>. Before joining HKUST, I received a Bachelor's degree in Automation from <a href="http://en.ustc.edu.cn/"></a>University of Science and Technology of China</a> in 2018.</p>
              <p style="text-align:center">
                <a href="mailto:zhuam@connect.ust.hk">Email</a> &nbsp/&nbsp
                <a href="data/CV_Zeyu_HU.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=2QstgG4AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/HUZeyu7">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/hzykent">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/zeyu_hu.jpg"><img style="width:70%;max-width:100%" alt="profile photo" src="images/zeyu_hu.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My primary research topic is about 3D scene understanding. Working on projects related to human pose estimation and generative models.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px ;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Teaser_Muma.png" alt="Muma_2025" width="200" height="" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a >
                <papertitle>MuMA: 3D PBR Texturing via Multi-Channel Multi-View Generation and Agentic Post-Processing</papertitle>
              </a>
              <br>
              Lingting Zhu, Jingrui Ye, Runze Zhang, <strong>Zeyu Hu</strong>, Yingda Yin, Lanjiong Li, Jinnan Chen, Shengju Qian, Xin Wang, Qingmin Liao, Lequan Yu
              <br>
              2025
              <br>
              <a href="https://arxiv.org/abs/2503.18461">paper</a> / 
              <a href="">code</a> /
              <a href="data/MuMA_2025.bib">bibtex</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mar3d.png" alt="MAR3D_CVPR2025" width="200" height="" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a >
                <papertitle>MAR-3D:Progressive Masked Auto-regressor for High-Resolution 3D Generation</papertitle>
              </a>
              <br>
              Jinnan Chen, Lingting Zhu, <strong>Zeyu Hu</strong>, Shengju Qian, Yugang Chen, Xin Wang, Gim Hee Lee
              <br>
              <em>CVPR</em>, 2025 (Highlight)
              <br>
              <a href="https://arxiv.org/abs/2503.20519">paper</a> / 
              <a href="https://github.com/jinnan-chen/MAR-3D">code</a> /
              <a href="data/MAR_CVPR2025.bib">bibtex</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/motionlab.png" alt="MotionLab_2025" width="200" height="" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a >
                <papertitle>MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm</papertitle>
              </a>
              <br>
              Ziyan Guo, <strong>Zeyu Hu</strong>, Na Zhao, De Wen Soh
              <br>
              2025
              <br>
              <a href="https://arxiv.org/abs/2502.02358">paper</a> / 
              <a href="https://github.com/Diouo/MotionLab">code</a> /
              <a href="data/MotionLab_2025.bib">bibtex</a>
            </td>
          </tr>          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/offts.png" alt="offts_2024" width="200" height="" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a >
                <papertitle>Off-the-shelf ChatGPT is a Good Few-shot Human Motion Predictor</papertitle>
              </a>
              <br>
              Haoxuan Qu, Zhaoyang He, <strong>Zeyu Hu</strong>, Yujun Cai, Jun Liu
              <br>
              2024
              <br>
              <a href="https://arxiv.org/abs/2405.15267">paper</a> / 
              <a href="">code</a> /
              <a href="data/offts_2024.bib">bibtex</a>
            </td>
          </tr>  
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/VMNet_TPAMI2022.png" alt="VMNet_TPAMI2022" width="200" height="" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="data/VMNet_TPAMI_merged.pdf">
                <papertitle>Voxel-Mesh Network for Geodesic-Aware 3D Semantic Segmentation of Indoor Scenes</papertitle>
              </a>
              <br>
              <strong>Zeyu Hu</strong>, Xuyang Bai, Jiaxiang Shang, Runze Zhang, Jiayu Dong, Xin Wang, Guangyuan Sun, Hongbo Fu, Chiew-Lan Tai
              <br>
              <em>TPAMI</em>, 2022 (ICCV 2021 SI invited)
              <br>
              <a href="data/VMNet_TPAMI_merged.pdf">paper</a> / 
              <a href="https://github.com/hzykent/VMNet">code</a> /
              <a href="data/VMNet_TPAMI_2022.bib">bibtex</a>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/LiDAL_ECCV2022.png" alt="LiDAL_ECCV2022" width="200" height="" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a >
                <papertitle>LiDAL: Inter-frame Uncertainty Based Active Learning for 3D LiDAR Semantic Segmentation</papertitle>
              </a>
              <br>
              <strong>Zeyu Hu</strong>, Xuyang Bai, Runze Zhang, Xin Wang, Guangyuan Sun, Hongbo Fu, Chiew-Lan Tai
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2211.05997">paper</a> / 
              <a href="https://github.com/hzykent/LiDAL">code</a> /  
              <a href="data/LiDAL_ECCV2022.bib">bibtex</a>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/TransFusion_CVPR2022.png" alt="SGMNet" width="200" height="" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2203.11496.pdf">
                <papertitle>TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with Transformers.</papertitle>
              </a>
              <br>
              Xuyang Bai, <strong>Zeyu Hu</strong>, Xinge Zhu, Qingqiu Huang, Yilun Chen, Hongbo Fu, Chiew-Lan Tai
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2203.11496.pdf">paper</a> / 
              <a href="https://github.com/XuyangBai/TransFusion">code</a> / 
              <a href="data/TransFusion_CVPR2021.bib">bibtex</a>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/VMNet_ICCV2021.png" alt="VMNet" width="200" height="">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2107.13824">
                <papertitle>VMNet: Voxel-Mesh Network for Geodesic-Aware 3D Semantic Segmentation</papertitle>
              </a>
              <br>
              <strong>Zeyu Hu</strong>, Xuyang Bai, Jiaxiang Shang, Runze Zhang, Jiayu Dong, Xin Wang, Guangyuan Sun, Hongbo Fu, Chiew-Lan Tai
              <br>
              <em>ICCV</em>, 2021 <a style="color:#FF0000";>(Oral)</a>
              <br>
              <a href="https://arxiv.org/pdf/2107.13824">paper</a> / 
              <a href="https://github.com/hzykent/VMNet">code</a> / 
              <a href="data/VMNet_ICCV2021.bib">bibtex</a>
              </p>
            </td>
          </tr>          


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/SGMNet_ICCV2021.png" alt="SGMNet" width="200" height="" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2108.08771.pdf">
                <papertitle>Learning to Match Features with Seeded Graph Matching Network.</papertitle>
              </a>
              <br>
              Hongkai Chen, Zixin Luo, Jiahui Zhang, Lei Zhou, Xuyang Bai, <strong>Zeyu Hu</strong>, Chiew-Lan Tai, Long Quan
              <br>
              <em>ICCV</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2108.08771.pdf">paper</a> / 
              <a href="https://github.com/vdvchen/SGMNet">code</a> / 
              <a href="data/SGMNet_ICCV2021.bib">bibtex</a>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/PointDSC_CVPR2021.png" alt="SGMNet" width="200" height="" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2103.05465">
                <papertitle>PointDSC: Robust Point Cloud Registration using Deep Spatial Consistency.</papertitle>
              </a>
              <br>
              Xuyang Bai, Zixin Luo, Lei Zhou, Hongkai Chen, Lei Li, <strong>Zeyu Hu</strong>, Hongbo Fu, Chiew-Lan Tai
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2103.05465.pdf">paper</a> / 
              <a href="https://github.com/XuyangBai/PointDSC">code</a> / 
              <a href="data/PointDSC_CVPR2021.bib">bibtex</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/JSENet_ECCV2020.png" alt="SGMNet" width="200" height="" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2007.06888">
                <papertitle>JSENet: Joint Semantic Segmentation and Edge Detection Network for 3D Point Clouds.</papertitle>
              </a>
              <br>
              <strong>Zeyu Hu</strong>, Mingmin Zhen, Xuyang Bai, Hongbo Fu, Chiew-lan Tai
              <br>
              <em>ECCV</em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/2007.06888.pdf">paper</a> / 
              <a href="https://github.com/hzykent/JSENet">code</a> / 
              <a href="data/JSENet_ECCV2020.bib">bibtex</a>
            </td>
          </tr>


        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
              <tr>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                      <heading>Experience</heading>
                      <p></p>
                      <li>Lightspeed Studios, Tencent, Aug.2020-Present</li>
                  </td>
              </tr>
          </tbody>
      </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>Reviewer Services</heading>
                        <p></p>
                        <li>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</li>
                        <li>European Conference on Computer Vision (ECCV)</li>
                        <li>Association for the Advancement of Artificial Intelligence (AAAI)</li>
                        <li>IEEE Transactions on Image Processing (TIP)</li>
                        <li>IEEE Transactions on Visualization and Computer Graphics (TVCG)</li>
                        <li>International Symposium on Mixed and Augmented Reality (ISMAR)</li>
                    </td>
                </tr>
            </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
              Thanks for the <a href="https://jonbarron.info/">template</a> of Jon Barron.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
